{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8862d89",
   "metadata": {},
   "source": [
    "# Implementation of Proximal Policy Optimization algorithm (PPO) - Paper: https://arxiv.org/abs/1707.06347¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfa4b9",
   "metadata": {},
   "source": [
    "### Note: Implementation is based on Github-User 'higgsfield'. <br>See: https://github.com/higgsfield/RL-Adventure-2/blob/master/3.ppo.ipynb. <br> Pseudocode can be found on: https://spinningup.openai.com/en/latest/algorithms/ppo.html. <br> Instead of seperate actor and critic network a shared network is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7760825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b151635",
   "metadata": {},
   "source": [
    "<h2> Define Hyperparameters </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 512 # Horizont (T) \n",
    "ADAM_LEARNING_RATE = 3e-4 # Adam Lernrate\n",
    "NUM_EPOCHS = 10 # Anzahl der Epochen\n",
    "MINI_BATCH_SIZE = 64 # Minibatch-Größe\n",
    "GAMMA_DISCOUNT = 0.99 # Diskontierungsfaktor (γ) \n",
    "LAMBDA_GAE = 0.95 # GAE Parameter (λ)\n",
    "C1_COEFFICIENT = 0.5 # C1 Koeffizent\n",
    "C2_COEFFICIENT = 0.001 # C2 Koeffizent\n",
    "CLIPPING = 0.2 # Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14485594",
   "metadata": {},
   "source": [
    "<h2> Another needed Parameters </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f722d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If gpu or cpu is used\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"For the neural network: {DEVICE} is used.\")\n",
    "# Hidden size of neural network\n",
    "HIDDEN_SIZE_1 = 128\n",
    "HIDDEN_SIZE_2 = 64\n",
    "# How long the algorithm should run\n",
    "MAX_STEPS = 15000000000000\n",
    "# Path to log th loss while running the algorithm. The Content in the file should be deleted for every run. \n",
    "# Else you have entrys from last run\n",
    "LOSS_LOG_PATH = \"./_logs/loss/loss.csv\"\n",
    "# Path to save model parameter while running the algorithm.\n",
    "MODEL_PATH = \"./_logs/models/\"\n",
    "# Path to save rewards while running the algorithm.\n",
    "REWARDS_PATH = \"./_logs/rewards/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9957b",
   "metadata": {},
   "source": [
    "<h2>Initialize environment</h2>\n",
    "If an error occurs. Make sure to change directory to gym_turtlebot3_wrapper and call pip install -e . <br>\n",
    "For more information read: <a href=\"https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa\">medium.com</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aad52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_turtlebot3.envs:turtlebot3-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7174d",
   "metadata": {},
   "source": [
    "<h2> Define NN for PPO-Clip-Algorithm </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size1=HIDDEN_SIZE_1, hidden_size2=HIDDEN_SIZE_2, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size1),\n",
    "            nn.LayerNorm(hidden_size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.LayerNorm(hidden_size2),\n",
    "            nn.Tanh(),    \n",
    "            nn.Linear(hidden_size2, 1),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size1),\n",
    "            nn.LayerNorm(hidden_size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.LayerNorm(hidden_size2),\n",
    "            nn.Tanh(),    \n",
    "            nn.Linear(hidden_size2, num_outputs),\n",
    "            nn.Tanh()   \n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c025549",
   "metadata": {},
   "source": [
    "<h2>Generalized Advantage Estimation (GAE)</h2>\n",
    "<h2><a href=\"https://arxiv.org/abs/1506.02438\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e923cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=GAMMA_DISCOUNT, tau=LAMBDA_GAE):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ea620",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1583859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=CLIPPING):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            # Normalize Advantage\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-10)\n",
    "            # Get distribution and value\n",
    "            dist, value = model(state)\n",
    "            # Get Entropy\n",
    "            entropy = dist.entropy().mean()\n",
    "            # Get log_prob\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "            # Calculate ratio\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            # Calculate clipping\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "            # Calculate actor and critic loss\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "            # Calculate total loss            \n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "            \n",
    "            # Write loss to log \n",
    "            with open(LOSS_LOG_PATH, 'a') as f:\n",
    "                f.write(f'{actor_loss}, {critic_loss}, {loss}')\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "            # Update neural network\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f2b98",
   "metadata": {},
   "source": [
    "<h2> Create neural network </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input shape for neural network (number of observations)\n",
    "num_inputs  = env.observation_space.shape[0]\n",
    "# Get output shape for neural netowrk (number of actions)\n",
    "num_outputs = env.action_space.shape[0]\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=ADAM_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100e869",
   "metadata": {},
   "source": [
    "<h2> Run PPO-Clip-Algorithm </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24889fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame_idx  = 0\n",
    "\n",
    "# list for logging\n",
    "steps_so_far = []\n",
    "rewards_so_far = []\n",
    "df = pd.DataFrame(columns = [\"step\",\"reward\"])\n",
    "\n",
    "# Run algorithm\n",
    "state = np.expand_dims(env.reset(), axis=0)\n",
    "\n",
    "while frame_idx < MAX_STEPS:\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(HORIZON):\n",
    "        # Get State\n",
    "        state = torch.FloatTensor(state).to(DEVICE) \n",
    "        # Get distribution and value\n",
    "        dist, value = model(state)\n",
    "        # Sample Action\n",
    "        action = dist.sample()\n",
    "        # Do step\n",
    "        next_state, reward, done, info = env.step(action.cpu().numpy())\n",
    "        # Save if done and next state\n",
    "        done = np.expand_dims(np.array(done), axis=0)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        # Get log_prob of action\n",
    "        log_prob = dist.log_prob(action)\n",
    "        # Get entropy of the distibution\n",
    "        entropy += dist.entropy().mean()\n",
    "        # Save Information about the step\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        reward = np.array([reward])\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(DEVICE))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(DEVICE))\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        # Append number of steps so far and rewards so far for logging,\n",
    "        steps_so_far.append(len(steps_so_far) + 1)\n",
    "        rewards_so_far.append(reward[0])\n",
    "        \n",
    "    # Set velocity and angular to zero while Updating network. \n",
    "    # If missing Turtlebot3 will drive while Updating the network, which can cause errors in the simulation \n",
    "    env.step(np.array([[0.0, 0.0]]))\n",
    "    next_state = torch.FloatTensor(next_state).to(DEVICE)\n",
    "    _, next_value = model(next_state)\n",
    "    # Calculate Return with GAE\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    # Calculate Advantage\n",
    "    advantage = returns - values\n",
    "    # Update neural network\n",
    "    ppo_update(NUM_EPOCHS, MINI_BATCH_SIZE, states, actions, log_probs, returns, advantage)\n",
    "    \n",
    "    \n",
    "    print(f\"Updating network: {datetime.now().strftime('%d-%m-%Y %H:%M:%S')} Steps so far: {len(steps_so_far)}\")\n",
    "    # Save model    \n",
    "    model_path = os.path.join(MODEL_PATH, f\"model_{len(steps_so_far)}_{datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\")\n",
    "    torch.save(model.state_dict(),model_path)\n",
    "    # Save rewards\n",
    "    df = pd.DataFrame(list(zip(steps_so_far, rewards_so_far)),columns =['step', 'reward'])\n",
    "    rewards_path = os.path.join(REWARDS_PATH,f\"{len(steps_so_far)}_{datetime.now().strftime('%d-%m-%Y %H:%M:%S')}.xlsx\")\n",
    "    df.to_excel(rewards_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4a6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
