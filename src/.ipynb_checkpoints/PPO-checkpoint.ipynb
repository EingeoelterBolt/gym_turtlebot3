{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7760825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import gym\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f496301",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66a3fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9957b",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8aad52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'model': 'waffle_pi', 'worlds': {'turtlebot3_world': {'target': {'x': 2.0, 'y': 0.0, 'z': 0.0}, 'number_of_lasers': 18, 'default': {'position': {'x': -2.0, 'y': -0.5, 'z': 0.0}, 'quaternion_orientation': {'x': 2.0, 'y': 0.0, 'z': 5.0, 'w': 1.0}}}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/Downloads/ros2-public-refactor/venv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('gym_turtlebot3.envs:turtlebot3-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7174d",
   "metadata": {},
   "source": [
    "# Define NN for PPO AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size1=128, hidden_size2=64, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size1),\n",
    "            nn.LayerNorm(hidden_size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.LayerNorm(hidden_size2),\n",
    "            nn.Tanh(),    \n",
    "            nn.Linear(hidden_size2, 1),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size1),\n",
    "            nn.LayerNorm(hidden_size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.LayerNorm(hidden_size2),\n",
    "            nn.Tanh(),    \n",
    "            nn.Linear(hidden_size2, num_outputs),\n",
    "            nn.Tanh()   \n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c025549",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e923cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ea620",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1583859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    #loss_df = pd.DataFrame(columns = [\"actor\",\"critic\",\"combined\"])\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            #print(f\"state shape: {state.shape}\")\n",
    "            # Eingefüte Zeule: Advantage normalization can später gelöschjt werden\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-10)\n",
    "            #print(f\"state: {state}\")\n",
    "            dist, value = model(state)\n",
    "            #print(f\"dist: {dist}\")\n",
    "            #print(f\"value shape: {value.shape}\")\n",
    "            #print(f\"value: {value}\")\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "            #print(f\"entropy: {entropy}\")\n",
    "            #print(f\"new_log_probs shape: {new_log_probs.shape}\")\n",
    "            #print(f\"new_log_probs: {new_log_probs}\")\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            #print(f\"ratio: {ratio}\")\n",
    "            surr1 = ratio * advantage\n",
    "            #print(f\"surr1: {surr1}\")\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "            #print(f\"surr2 {surr2}\")\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "            #print(f\"actor_loss: {actor_loss}\")\n",
    "            #print(f\"critic_loss: {critic_loss}\")\n",
    "            \n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "            #print(f\"loss: {loss}\")\n",
    "            \n",
    "            with open('loss.csv', 'a') as f:\n",
    "                f.write(f'{actor_loss}, {critic_loss}, {loss}')\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 512\n",
    "mini_batch_size  = 64\n",
    "ppo_epochs       = 10\n",
    "threshold_reward = -200\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c35c70",
   "metadata": {},
   "source": [
    "model.load_state_dict(torch.load('model_100864_26-08-2021 07:36:01'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0939f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 15000000000000\n",
    "frame_idx  = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24889fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = np.expand_dims(env.reset(), axis=0)\n",
    "early_stop = False\n",
    "\n",
    "steps_so_far = []\n",
    "rewards_so_far = []\n",
    "df = pd.DataFrame(columns = [\"step\",\"reward\"])\n",
    "\n",
    "ac_loss = []\n",
    "cr_loss = []\n",
    "co_loss = [] \n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device) \n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, info = env.step(action.cpu().numpy())\n",
    "        done = np.expand_dims(np.array(done), axis=0)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        reward = np.array([reward])\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "         # Append number of steps so far\\n\",\n",
    "        steps_so_far.append(len(steps_so_far) + 1)\n",
    "        rewards_so_far.append(reward[0])\n",
    "        \n",
    "        \"\"\"\n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env() for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "        \"\"\"    \n",
    "    env.step(np.array([[0.0, 0.0]]))\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
    "    print(f\"Updating network: {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\")\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f\"model_{len(steps_so_far)}_{datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\")\n",
    "    df = pd.DataFrame(list(zip(steps_so_far, rewards_so_far)),columns =['step', 'reward'])\n",
    "    df.to_excel(f\"versuch_1/log/{len(steps_so_far)}_{datetime.now().strftime('%d-%m-%Y %H:%M:%S')}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4dd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6238f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
